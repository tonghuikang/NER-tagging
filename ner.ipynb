{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Identify if a sentence mentions Donald Trump is any form:\n",
    "\n",
    ">POTUS, POTUS 45, Trump, US President, Commander-in-Chief. <BR>\n",
    "\n",
    "and not these:\n",
    "\n",
    "> Obama, POTUS 44, Commander-in-Chief of Canada \n",
    "\n",
    "And the see if this can be extended to the local context. For example Lee Hsien Loong should be identified by\n",
    "\n",
    "> LHL, PM Lee, Prime Minister of Singapore, Prime Minister of the Republic of Singapore, <BR>\n",
    "> Prime Minister (for article written in the context of Singapore)\n",
    "\n",
    "### Installation script\n",
    "```\n",
    "conda create -n ner python=3.6\n",
    "source activate ner\n",
    "conda install -c conda-forge spacy -y\n",
    "conda install ipython jupyter nb_conda nltk -y\n",
    "python -m spacy download en\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "### Summary\n",
    "At the very list it can identify the people involved in each `claimReviewed` and the compare with every sentence in the article.\n",
    "\n",
    "I still has yet to understand how a model could be trained to suit Singapore's context. https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code from https://spacy.io/usage/linguistic-features#section-named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This identified entities, the position from the sentence and its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify each word that made up the entity is before or after, nothing quite new here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 0, 13, 'GPE'), ('Facebook', 32, 40, 'ORG'), ('Lee Hsien Loong', 46, 61, 'PERSON'), ('Singapore', 91, 100, 'GPE'), ('Lee', 123, 126, 'PERSON'), ('Lee Hsien Loong', 146, 161, 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'''San Francisco considers banning Facebook, and Lee Hsien Loong who is \\\n",
    "the Prime Minister of Singapore is concerned about PM Lee and Prime Minister Lee Hsien Loong. ''')\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Trump', 23, 28, 'PERSON'), ('Trump Jr.', 41, 50, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'''We know that President Trump is evil and Trump Jr. is involved in illegal activites.''')\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['San', 'B', 'GPE']\n",
      "['Francisco', 'I', 'GPE']\n"
     ]
    }
   ],
   "source": [
    "# token level\n",
    "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "print(ent_san)  # [u'San', u'B', u'GPE']\n",
    "print(ent_francisco)  # [u'Francisco', u'I', u'GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN The first token of a multi-token entity.\n",
    "# IN An inner token of a multi-token entity.\n",
    "# LAST The final token of a multi-token entity.\n",
    "# UNIT A single-token entity.\n",
    "# OUT A non-entity token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to make the algorithm recognise `FB` is a entity\n",
    "\n",
    "\"To ensure that the sequence of token annotations remains consistent, you have to set entity annotations at the document level.\"\n",
    "\n",
    "Thoughts - we could perhaps customise this for Singapore. Recognise all the notable indivuduals and their acronyms. Is there a way equate relationships - such as Prime Minister of Singapore to Lee Hsien Loong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"FB is hiring a new Vice President of global policy from San Francisco. FB is evil.\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# the model didn't recognise \"FB\" as an entity :(\n",
    "\n",
    "ORG = doc.vocab.strings[u'ORG']  # get hash value of entity label\n",
    "fb_ent = Span(doc, 0, 1, label=ORG) # create a Span for the new entity\n",
    "# assigns FB with the entity 'doc'\n",
    "doc.ents = list(doc.ents) + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('After', ents)\n",
    "# [(u'FB', 0, 2, 'ORG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERSON\tPeople, including fictional.\n",
    "# NORP\tNationalities or religious or political groups.\n",
    "# FAC\tBuildings, airports, highways, bridges, etc.\n",
    "# ORG\tCompanies, agencies, institutions, etc.\n",
    "# GPE\tCountries, cities, states.\n",
    "# LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "# PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "# EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "# WORK_OF_ART\tTitles of books, songs, etc.\n",
    "# LAW\tNamed documents made into laws.\n",
    "# LANGUAGE\tAny named language.\n",
    "# DATE\tAbsolute or relative dates or periods.\n",
    "# TIME\tTimes smaller than a day.\n",
    "# PERCENT\tPercentage, including \"%\".\n",
    "# MONEY\tMonetary values, including unit.\n",
    "# QUANTITY\tMeasurements, as of weight or distance.\n",
    "# ORDINAL\t\"first\", \"second\", etc.\n",
    "# CARDINAL\tNumerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like designing a NER algorithm make a doc given all the tags\n",
    "But how to use it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original [London, San Francisco, London]\n",
      "Before []\n",
      "After [London]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import spacy\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'London is a big city in the San Francisco. London is trashy.')\n",
    "print('Original', list(doc.ents))\n",
    "\n",
    "doc = nlp.make_doc(u'London is a big city in the San Francisco. London is trashy.')\n",
    "print('Before', list(doc.ents))  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = numpy.zeros((len(doc), len(header)))\n",
    "# len(doc) is the number of tokens\n",
    "\n",
    "# the first zero refer to the sentence.\n",
    "attr_array[0, 0] = 3  # 3 is begin, 1 is inside, 2 is outside\n",
    "attr_array[0, 1] = doc.vocab.strings[u'GPE']\n",
    "\n",
    "doc.from_array(header, attr_array)\n",
    "# load the attribute array to the doc\n",
    "print('After', list(doc.ents))  # [London]\n",
    "\n",
    "doc = nlp(u'London is a big city in the San Francisco. London is trashy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'ent' visualizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [19/Aug/2018 22:49:02] \"GET / HTTP/1.1\" 200 3336\n",
      "127.0.0.1 - - [19/Aug/2018 22:49:02] \"GET /favicon.ico HTTP/1.1\" 200 3336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"\"\"But Google is starting from behind. The company made a late push \\\n",
    "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa \\\n",
    "software, which runs on its Echo and Dot devices, have clear leads in \\\n",
    "consumer adoption.\"\"\"\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See result on http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something related to training\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['O', 'O', 'U-LOC', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# I don't understand what this does\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "\n",
    "doc = nlp.make_doc(u'I like London. London is amazing.')\n",
    "print(list(doc.ents))\n",
    "entities = [(7, 13, 'LOC')]\n",
    "tags = biluo_tags_from_offsets(doc, entities)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [('Who is Chaka Khan?', [(7, 17, 'PERSON')]),\n",
    "              ('I like London and Berlin.', [(7, 13, 'LOC'), (18, 24, 'LOC')])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "doc = Doc(nlp.vocab, [u'rats', u'make', u'good', u'pets'])\n",
    "gold = GoldParse(doc, entities=[u'U-ANIMAL', u'O', u'O', u'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"\"\"But Google is starting from behind. The company made a late push\n",
    "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
    "software, which runs on its Echo and Dot devices, have clear leads in\n",
    "consumer adoption.\"\"\"\n",
    "\n",
    "nlp = spacy.load('custom_ner_model')\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ner]",
   "language": "python",
   "name": "conda-env-ner-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
